{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h9464I-uxLiw"
   },
   "source": [
    "# Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dg2ooa4DxLiz"
   },
   "source": [
    "## Task-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OnV82tg1xLi0"
   },
   "source": [
    "### Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bUsYm9wjxLi1"
   },
   "outputs": [],
   "source": [
    "## SkLearn# Collection of string documents\n",
    "corpus = ['this is the first document',\n",
    "          'this document is the second document',\n",
    "          'and this is the third one',\n",
    "          'is this the first document' ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eLwmFZfKxLi4"
   },
   "source": [
    "## SkLearn Implementation\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Np4dfQOkxLi4",
    "outputId": "807c7176-86ad-45b9-9c78-3ab4a90760a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(corpus)\n",
    "print(vectorizer.get_feature_names())   # sklearn feature names, they are sorted in alphabetic order by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "dTKplK96xLi-",
    "outputId": "295058cc-a3b7-4ed3-a5ff-89ffd5b886d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.91629073 1.22314355 1.51082562 1.         1.91629073 1.91629073\n",
      " 1.         1.91629073 1.        ]\n"
     ]
    }
   ],
   "source": [
    "# Here we will print the sklearn tfidf vectorizer idf values after applying the fit method\n",
    "# After using the fit function on the corpus the vocab has 9 words in it, and each has its idf value.\n",
    "\n",
    "print(vectorizer.idf_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "-CTiWHygxLjA",
    "outputId": "962798c0-376a-4fc3-f9d9-2196e6491faf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 9)"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape of sklearn tfidf vectorizer output after applying transform method.\n",
    "skl_output = vectorizer.transform(corpus)\n",
    "skl_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "bDKEpbA-xLjD",
    "outputId": "65085f16-7229-483d-950e-50b6cbc404ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 8)\t0.38408524091481483\n",
      "  (0, 6)\t0.38408524091481483\n",
      "  (0, 3)\t0.38408524091481483\n",
      "  (0, 2)\t0.5802858236844359\n",
      "  (0, 1)\t0.46979138557992045\n"
     ]
    }
   ],
   "source": [
    "# sklearn tfidf values for first line of the above corpus.\n",
    "# Here the output is a sparse matrix\n",
    "\n",
    "print(skl_output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "3QWo34hexLjF",
    "outputId": "832cba23-fae0-407c-cb42-8267eb30ffe0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
      "  0.38408524 0.         0.38408524]]\n"
     ]
    }
   ],
   "source": [
    "# sklearn tfidf values for first line of the above corpus.\n",
    "# To understand the output better, here we are converting the sparse output matrix to dense matrix and printing it.\n",
    "# Notice that this output is normalized using L2 normalization. sklearn does this by default.\n",
    "\n",
    "print(skl_output[0].toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qfIwx5LzxLjI"
   },
   "source": [
    "## My custom implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7gRQNfZhFtHy"
   },
   "outputs": [],
   "source": [
    "# Required imports for my custom Implementation\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from scipy.sparse import csr_matrix\n",
    "import math\n",
    "import operator\n",
    "from sklearn.preprocessing import normalize\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "NGaju_zkFzni",
    "outputId": "f4a54336-7bbe-45bf-ebc6-40482200daf8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n"
     ]
    }
   ],
   "source": [
    "# Fit fucntion to define the features (unique words)\n",
    "def fit(dataset):    \n",
    "    unique_words = set() # at first we will initialize an empty set\n",
    "    # check if its list type or not\n",
    "    if type(dataset) == list:\n",
    "        for review in dataset: # for each review in the dataset\n",
    "            for word in review.split(\" \"): # for each word in the review. #split method converts each string into list of words\n",
    "                if len(word) >= 2:\n",
    "                    unique_words.add(word)\n",
    "        return sorted(list(unique_words))\n",
    "    else:\n",
    "        print(\"Invalid Input. Please give corpus as a list\")\n",
    "My_Custom_features = fit(corpus)\n",
    "print(My_Custom_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "hgI8a1JaGIVK",
    "outputId": "2cc12c02-b279-49cf-e795-4f3474dfd146"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.916290731874155,\n",
       " 1.2231435513142097,\n",
       " 1.5108256237659907,\n",
       " 1.0,\n",
       " 1.916290731874155,\n",
       " 1.916290731874155,\n",
       " 1.0,\n",
       " 1.916290731874155,\n",
       " 1.0]"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# function to calculate IDF vale of any word in given corpus\n",
    "def IDF(t, dataset):\n",
    "    n = 0              #Number_of_documents_having_t = 0 \n",
    "    N = len(dataset)   # total number of Docs in corpes\n",
    "    for review in dataset: # for each review in the dataset\n",
    "        for word in review.split(\" \"): # for each word in the review. #split method converts each string into list of words\n",
    "            if word ==t:\n",
    "                n +=1 ; \n",
    "                break   # goes to nextsentence at first occurance of the t in a document\n",
    "    return 1+math.log((1+N)/(1+n))\n",
    "\n",
    "idf = [IDF(word, corpus) for word in My_Custom_features] #list of IDF value in same order as vocab\n",
    "idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "Xvb4LxYjGWBy",
    "outputId": "4e12c8f2-e85d-48ea-b353-5fb4e4dd26be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1)\t0.4697913855799205\n",
      "  (0, 2)\t0.5802858236844359\n",
      "  (0, 3)\t0.3840852409148149\n",
      "  (0, 6)\t0.3840852409148149\n",
      "  (0, 8)\t0.3840852409148149\n"
     ]
    }
   ],
   "source": [
    "# final custom tf-idf Vectorizer\n",
    "def transform(Corpus, Dimensions):\n",
    "    loc_r, loc_c, values = [], [], []\n",
    "\n",
    "    if type(Corpus) == list:\n",
    "        r = 0    # to store row index in sparse matric\n",
    "        for review in Corpus:       # sets row\n",
    "            word_freq = dict(Counter(review.split()))  # counts occurance of a word in a doc and stores as dictionary[word] = count\n",
    "            c = 0    # to store col index in sparse matric\n",
    "            for word in Dimensions:       # sets column\n",
    "                if word in review.split(): # checks if the word is in present row\n",
    "                    loc_r.append(r) ; loc_c.append(c)\n",
    "                    values.append(((word_freq[word])*IDF(word, corpus))/len(review.split()))\n",
    "                c +=1\n",
    "            r +=1\n",
    "        return normalize(csr_matrix((values, (loc_r, loc_c)), shape=(len(Corpus),len(Dimensions))), \"l2\")\n",
    "    else:\n",
    "        print(\"you need to pass list of strings\")\n",
    "\n",
    "My_Custom_Output = transform(corpus,My_Custom_features)\n",
    "My_Custom_Output.shape\n",
    "print(My_Custom_Output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "WinwxLSHHcUQ",
    "outputId": "510cb576-aedd-4df1-9eb7-aef45e90fc80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
      "  0.38408524 0.         0.38408524]]\n"
     ]
    }
   ],
   "source": [
    "print(My_Custom_Output[0].toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MMxBmVZExLjK"
   },
   "source": [
    "## Task-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yRwZnYc2IBs6"
   },
   "outputs": [],
   "source": [
    "# Required imports for my custom Implementation\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from scipy.sparse import csr_matrix\n",
    "import math\n",
    "import operator\n",
    "from sklearn.preprocessing import normalize\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "NHxPLlwNxLjL",
    "outputId": "3a274c5e-55d7-459f-8d1c-751c94a5985a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents in corpus =  746\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open('/content/drive/My Drive/Colab Notebooks/3_CountVectorizer/cleaned_strings', 'rb') as f:\n",
    "    corpus_2 = pickle.load(f)\n",
    "\n",
    "# printing the length of the corpus loaded\n",
    "print(\"Number of documents in corpus = \",len(corpus_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ZULfoOIdxLjQ",
    "outputId": "6ab3c564-20ee-4173-9f30-4984749700e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2886\n"
     ]
    }
   ],
   "source": [
    "# fit function to return vocab as unique values\n",
    "def fit(Corpus):    \n",
    "    unique_words = set() # at first we will initialize an empty set\n",
    "    # check if its list type or not\n",
    "    if type(Corpus) == list:\n",
    "        for review in Corpus: # for each review in the Corpus\n",
    "            for word in review.split(\" \"): # for each word in the review. #split method converts each string into list of words\n",
    "                if len(word) >= 2:\n",
    "                    unique_words.add(word)\n",
    "        return sorted(list(unique_words))\n",
    "    else:\n",
    "        print(\"Invalid Input. Please give corpus as a list\")\n",
    "vocab = fit(corpus_2)\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "1_DJnnR3xLjR",
    "outputId": "96d2d643-f21c-4750-b3a8-a5d4006ebeeb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2886\n",
      "[6.922918004572872, 6.922918004572872, 6.229770824012927, 6.922918004572872, 5.3134800921387715]\n"
     ]
    }
   ],
   "source": [
    "def IDF(t, Corpus):\n",
    "    n = 0              #Number_of_documents_having_t = 0 \n",
    "    N = len(Corpus)   # total number of Docs in corpes\n",
    "    for doc in Corpus: # for each doc in the Corpus\n",
    "        for word in doc.split(\" \"): # for each word in the doc. #split method converts each string into list of words\n",
    "            if word ==t:\n",
    "                n +=1 ; \n",
    "                break   # goes to nextsentence at first occurance of the t in a document\n",
    "    return 1+math.log((1+N)/(1+n))\n",
    "\n",
    "list_idf = [IDF(word, corpus_2) for word in vocab] #list of IDF value in same order as vocab\n",
    "print(len(list_idf))\n",
    "print(list_idf[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "XveRxUVIZYTv",
    "outputId": "df768dff-c35e-4cf1-8240-67cb716c4d97"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aailiyah', 'abandoned', 'ability', 'abroad', 'absolutely', 'abstruse', 'abysmal', 'academy', 'accents', 'accessible', 'acclaimed', 'accolades', 'accurate', 'accurately', 'accused', 'achievement', 'achille', 'ackerman', 'act', 'acted', 'acting', 'action', 'actions', 'actor', 'actors', 'actress', 'actresses', 'actually', 'adams', 'adaptation', 'add', 'added', 'addition', 'admins', 'admiration', 'admitted', 'adorable', 'adrift', 'adventure', 'advise', 'aerial', 'aesthetically', 'affected', 'affleck', 'afraid', 'africa', 'afternoon', 'age', 'aged', 'ages']\n"
     ]
    }
   ],
   "source": [
    "# ordered idf values for alphabetical words in the vocab\n",
    "idf_vocab = tuple(zip(vocab, list_idf))    # tuple of word along with its idf value as (word, idf)\n",
    "list_idf.sort(reverse = True)              # sorting idf values in descending order\n",
    "Rank_list = []\n",
    "for idf in list_idf:\n",
    "    for entry in idf_vocab:\n",
    "        if entry[0] in Rank_list:\n",
    "            continue\n",
    "        if entry[1] == idf:\n",
    "            Rank_list.append(entry[0]) ; break\n",
    "Dimensions = Rank_list[:50]          # Top 50 idf words as dimensions\n",
    "print(Dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "XJiLPt8fxoGi",
    "outputId": "27d97407-7f69-4706-c658-2a18af8b3921"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(746, 50)"
      ]
     },
     "execution_count": 25,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# final custom tf-idf Vectorizer\n",
    "def transform(Corpus, Dimensions):\n",
    "    loc_r, loc_c, values = [], [], []\n",
    "    if type(Corpus) == list:\n",
    "        r = 0\n",
    "        for doc in Corpus: # for each document in the Corpus\n",
    "            word_freq = dict(Counter(doc.split())) ; c = 0\n",
    "            for word in Dimensions: \n",
    "                if word in doc.split():\n",
    "                    \n",
    "                    loc_r.append(r) ; loc_c.append(c)\n",
    "                    values.append(((word_freq[word])*IDF(word, Corpus))/len(doc.split()))\n",
    "                c +=1\n",
    "            r +=1\n",
    "        normalized_sparse_matrix = normalize(csr_matrix((values, (loc_r, loc_c)), shape=(len(Corpus),len(Dimensions))), \"l2\")\n",
    "        return normalized_sparse_matrix\n",
    "    else:\n",
    "        print(\"you need to pass list of strings\")\n",
    "Custom_TF_IDF = transform(corpus_2,Dimensions)\n",
    "print(Custom_TF_IDF[10].toarray())\n",
    "Custom_TF_IDF.shape"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "eLwmFZfKxLi4",
    "qfIwx5LzxLjI"
   ],
   "name": "Copy of Assignment_3_Instructions.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
